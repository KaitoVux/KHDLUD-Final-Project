{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regex\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "import urllib.request\n",
    "import joblib #load, dump pkl\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from underthesea import word_tokenize #word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers as ppb # load model BERT\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lấy dữ liệu comment từ  URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllComment(url):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.maximize_window()\n",
    "\n",
    "    records = []\n",
    "    browser.get(url)\n",
    "    # tat quang cao\n",
    "    browser.find_element_by_css_selector('html').send_keys(Keys.ESCAPE)\n",
    "\n",
    "    # scroll\n",
    "    time.sleep(1)\n",
    "    total_height = int(browser.execute_script(\n",
    "        \"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, 5):\n",
    "        browser.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "    time.sleep(3)\n",
    "\n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    button = browser.find_elements_by_css_selector(\n",
    "        'button.align-right.secondary.slidedown-button')\n",
    "\n",
    "    if(len(button) > 0):\n",
    "        button[0].click()\n",
    "\n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    commentDivs = soup.findAll(\n",
    "        \"div\", {\"class\": \"style__StyledComment-sc-103p4dk-5 dDtAUu review-comment\"})\n",
    "    for commentDiv in commentDivs:\n",
    "        cmt = commentDiv.find(\n",
    "            \"div\", {\"class\": \"review-comment__content\"}).text\n",
    "        if(len(cmt) > 0):\n",
    "            records.append(cmt)\n",
    "\n",
    "    navigationButton = browser.find_elements_by_css_selector(\n",
    "        'a.btn.next')\n",
    "\n",
    "    while len(navigationButton) > 0:\n",
    "        navigationButton[0].click()\n",
    "        time.sleep(1)\n",
    "        html_source = browser.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "        commentDivs = soup.findAll(\n",
    "            \"div\", {\"class\": \"style__StyledComment-sc-103p4dk-5 dDtAUu review-comment\"})\n",
    "        for commentDiv in commentDivs:\n",
    "            cmt = commentDiv.find(\n",
    "                \"div\", {\"class\": \"review-comment__content\"}).text\n",
    "            if(len(cmt) > 0):\n",
    "                records.append(cmt)\n",
    "        navigationButton = browser.find_elements_by_css_selector(\n",
    "            'a.btn.next')\n",
    "    browser.quit()\n",
    "\n",
    "    return {'comment':records}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tách các comment thành từ hoặc cụm từ có nghĩa, fomart=\"text\" sẽ cho ra kết quả các từ đơn và từ ghép có nghĩa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(row):\n",
    "    return word_tokenize(row, format=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuẩn hoá comment\n",
    "Dữ liệu hiện đang chứa những kí tự như dấu chấm câu, kí tự đặc biệt, icon,... Những kí tự này không cần thiết cho việc khai phá dữ liệu nên ta sẽ tiến hành loại bỏ chúng. Các bước loại bỏ:\n",
    "- Xoá đi các dấu chấm câu như (. , ? ; ...)\n",
    "- Xoá đi các kí tự đặc biệt: @,#,*,...\n",
    "- Xoá khoảng trắng đầu và cuối mỗi comment, xoá  khoảng trắng dư thừa giữa các từ.\n",
    "- Xoá đi icons và emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(row):\n",
    "    row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
    "    row = re.sub(r'[^\\w]',' ', row) \n",
    "    row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
    "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
    "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "        .replace(\"-\", \" \").replace(\"?\", \" \")\n",
    "    row = row.strip()\n",
    "    row = \" \".join(row.split())\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hàm xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chuẩn hoá sẽ thực hiện các công việc sau:\n",
    "- Chuẩn hoá comment dùng hàm standardize_data ở trên.\n",
    "- Loại bỏ các comment trùng lặp.\n",
    "- Tokenizer các comment.\n",
    "- Trả về Series comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(data):\n",
    "    # 1. Standardize data\n",
    "    data_frame = pd.DataFrame(data=data)\n",
    "    data_frame.drop_duplicates(subset =\"comment\", keep = 'first', inplace = True) \n",
    "    data_frame['comment'] = data_frame['comment'].apply(standardize_data)\n",
    "    # 2. Tokenizer\n",
    "    data_frame['comment'] = data_frame['comment'].apply(tokenizer)\n",
    "    # 3. Embedding\n",
    "    X_val = data_frame['comment']\n",
    "    print('comments')\n",
    "    print(X_val)\n",
    "    return X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-train model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrainModel(data):\n",
    "    '''\n",
    "    Load pretrain model/ tokenizers\n",
    "    Return : features\n",
    "    '''\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    #encode lines\n",
    "    tokenized = data.apply((lambda x: tokenizer.encode(x, add_special_tokens = True,max_length=512,truncation=True)))\n",
    "\n",
    "    # get lenght max of tokenized\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    print('max len:', max_len)\n",
    "\n",
    "    # if lenght of tokenized not equal max_len , so padding value 0\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    print('padded:', padded[1])\n",
    "    print('len padded:', padded.shape)\n",
    "\n",
    "    #get attention mask ( 0: not has word, 1: has word)\n",
    "    attention_mask = np.where(padded ==0, 0,1)\n",
    "    print('attention mask:', attention_mask[1])\n",
    "\n",
    "    # Convert input to tensor\n",
    "    padded = torch.tensor(padded,dtype=torch.long)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "    #     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    print('features:', features)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hàm predict sẽ nhận vào url sản phẩm bất kì từ https://tiki.vn/. Tiến hành crawl các comment và chuẩn hoá dữ liệu. Sau đó đưa vào BERT model để rút trích features và đánh giá bằng model chúng ta đã train ở bước Training model.\n",
    "- Hàm anlyze sẽ nhận vào kết quả predict và đếm số lượng các loại bình luận được gán nhãn. Nếu bình luận tích cực có số lượng lớn ·hơn sẽ gợi ý người dùng mua sản phẩm và ngược lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(result):\n",
    "    good = np.count_nonzero(result)\n",
    "    bad = len(result) - good\n",
    "    \n",
    "    print(\"Số bình luận tích cực và trung lập: \", good)\n",
    "    print(\"Số bình luận tiêu cực: \", bad)\n",
    "\n",
    "    if good>bad:\n",
    "        return \"Mặt hàng tốt! Bạn có thể mua ngay.\"\n",
    "    else:\n",
    "        return \"Có nhiều bình luận xấu! Cần xem xét kĩ trước khi quyết định mua.\"\n",
    "\n",
    "def predict(url):\n",
    "    # 1. Load URL and print comments\n",
    "    data = getAllComment(url)\n",
    "    data = processing_data(data)\n",
    "    features = load_pretrainModel(data)\n",
    "    # 2. Load weights\n",
    "    model = joblib.load('save_model.pkl')\n",
    "    # 3. Result\n",
    "    result = model.predict(features)\n",
    "    print(result)\n",
    "    print(analyze(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy thử nghiệm quá trình gợi ý mua sản phẩm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tiki.vn/macbook-air-2017-mqd32-13-inch-hang-chinh-hang-p721995.html'\n",
    "predict(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
